{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu/M1hi5aIs52ShTbzTRl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gaulgeous/Energy-Forecasting/blob/main/final_rejig.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from sklearn.metrics import mean_squared_error as mse, r2_score, mean_absolute_error as mae, mean_absolute_percentage_error as mape\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import InputLayer, LSTM, Dense, Conv1D, Flatten, GRU, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "import absl.logging\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "import time\n",
        "\n",
        "import keras_tuner as kt\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_NL_C-DbLgJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits = 3\n",
        "cv_period = 90"
      ],
      "metadata": {
        "id": "L5I9MfSqQJn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(predictions, actual, cv, model_name):\n",
        "\n",
        "    MSE = mse(actual, predictions, squared=True)\n",
        "    MAE = mae(actual, predictions)\n",
        "    MAPE = mape(actual, predictions)\n",
        "    RMSE = mse(actual, predictions, squared=False)\n",
        "    R2 = r2_score(actual, predictions)\n",
        "    if cv:\n",
        "        metrics = {model_name + '_RMSE': RMSE, model_name + '_R2': R2, model_name +'_MSE': MSE,\n",
        "                   model_name + '_MAE': MAE, model_name + '_MAPE': MAPE}\n",
        "    else:\n",
        "        metrics = {'RMSE': RMSE, 'R2': R2, 'MSE': MSE, 'MAE': MAE, 'MAPE': MAPE}\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def cross_val_metrics(total_metrics, set_name, future, model_name):\n",
        "\n",
        "    csv_directory = os.getcwd() + \"/csvs\"\n",
        "    df = pd.DataFrame(total_metrics)\n",
        "    df.to_csv(csv_directory + \"/\" + set_name + \"_\" + model_name + \"_cv_metrics_\" + str(future) + \".csv\", index=False)\n",
        "\n",
        "\n",
        "def normalise_metrics(metrics, training):\n",
        "\n",
        "    rmse = [key[\"RMSE\"] for key in metrics]\n",
        "    mse = [key[\"MSE\"] for key in metrics]\n",
        "    mae = [key[\"MAE\"] for key in metrics]\n",
        "    r2 = [key[\"R2\"] for key in metrics]\n",
        "\n",
        "    metrics_sets = {\"RMSE\": rmse, \"MSE\": mse, \"MAE\": mae, \"R2\": r2}\n",
        "\n",
        "    if training:\n",
        "        time = [key[\"TIME\"] for key in metrics]\n",
        "        metrics_sets = {\"RMSE\": rmse, \"MSE\": mse, \"MAE\": mae, \"R2\": r2, \"TIME\": time}\n",
        "\n",
        "    for name, set in metrics_sets.items():\n",
        "        top = max(set)\n",
        "        counter = 0\n",
        "\n",
        "        while top > 10:\n",
        "            top /= 10\n",
        "            set = [entry / 10 for entry in set]\n",
        "            counter += 1\n",
        "\n",
        "        i = 0\n",
        "\n",
        "        for key in metrics:\n",
        "            key[name] = set[i]\n",
        "            i += 1\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# These next two functions are the writing functions. Not sure if they need re-jigging or if I should just fully send it\n",
        "def make_metrics_csvs(csv_directory, metrics, set_name, future, training):\n",
        "\n",
        "    for model_name, metric_outputs in metrics.items():\n",
        "\n",
        "        if not os.path.exists(csv_directory + \"/\" + set_name + \"_metrics_\" + str(future) + \".csv\"):\n",
        "            metrics = pd.DataFrame({\"Model\": [], \"Metric\": [], \"Value\": []})\n",
        "            metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"RMSE\", \"Value\": metric_outputs.get(\"RMSE\")}\n",
        "            metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"MAE\", \"Value\": metric_outputs.get(\"MAE\")}\n",
        "            metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"MAPE\", \"Value\": metric_outputs.get(\"MAPE\")}\n",
        "            metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"R2\", \"Value\": metric_outputs.get(\"R2\")}\n",
        "            if training:\n",
        "                metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"TIME\", \"Value\": metric_outputs.get(\"TIME\")}\n",
        "\n",
        "            metrics.to_csv(csv_directory + \"/\" + set_name + \"_metrics_\" + str(future) + \".csv\", index=False)\n",
        "        else:\n",
        "\n",
        "            metrics = pd.read_csv(csv_directory + \"/\" + set_name + \"_metrics_\" + str(future) + \".csv\")\n",
        "\n",
        "            if model_name in metrics['Model'].values:\n",
        "                metrics.loc[(metrics['Model'] == model_name) & (metrics[\"Metric\"] == \"RMSE\"), 'Value'] = metric_outputs.get(\"RMSE\")\n",
        "                metrics.loc[(metrics['Model'] == model_name) & (metrics[\"Metric\"] == \"MAE\"), 'Value'] = metric_outputs.get(\"MAE\")\n",
        "                metrics.loc[(metrics['Model'] == model_name) & (metrics[\"Metric\"] == \"MAPE\"), 'Value'] = metric_outputs.get(\"MAPE\")\n",
        "                metrics.loc[(metrics['Model'] == model_name) & (metrics[\"Metric\"] == \"R2\"), 'Value'] = metric_outputs.get(\"R2\")\n",
        "                if training:\n",
        "                    metrics.loc[(metrics['Model'] == model_name) & (metrics[\"Metric\"] == \"TIME\"), 'Value'] = metric_outputs.get(\"TIME\")\n",
        "            else:\n",
        "                metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"RMSE\", \"Value\": metric_outputs.get(\"RMSE\")}\n",
        "                metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"MAE\", \"Value\": metric_outputs.get(\"MAE\")}\n",
        "                metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"MAPE\", \"Value\": metric_outputs.get(\"MAPE\")}\n",
        "                metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"R2\", \"Value\": metric_outputs.get(\"R2\")}\n",
        "                if training:\n",
        "                    metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"TIME\", \"Value\": metric_outputs.get(\"TIME\")}\n",
        "            metrics.to_csv(csv_directory + \"/\" + set_name + \"_metrics_\" + str(future) + \".csv\", index=False)\n",
        "\n",
        "\n",
        "def make_csvs(csv_directory, predictions, y_test, pred_dates_test, set_name, future, model_name):\n",
        "\n",
        "    if not os.path.exists(csv_directory + \"/\" + set_name + \"_performances_\" + str(future) + \".csv\"):\n",
        "        performances = pd.DataFrame({\"Date\":pred_dates_test, \"Actual\": y_test.flatten(), model_name: predictions.flatten()})\n",
        "        performances = performances.iloc[-1000:,:]\n",
        "        performances.to_csv(csv_directory + \"/\" + set_name + \"_performances_\" + str(future) + \".csv\", index=False)\n",
        "    else:\n",
        "        performances = pd.read_csv(csv_directory + \"/\" + set_name + \"_performances_\" + str(future) + \".csv\")\n",
        "        performances[model_name] = predictions[-1000:]\n",
        "        performances.to_csv(csv_directory + \"/\" + set_name + \"_performances_\" + str(future) + \".csv\", index=False)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Z9VLwmR7LxQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_baseline_model():\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(1, 'linear'))\n",
        "\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001),\n",
        "                metrics=['mean_squared_error'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_simple_model():\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "\n",
        "    model.add(Dense(1, 'linear'))\n",
        "\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001),\n",
        "                metrics=['mean_squared_error'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Testing\n",
        "def train_simple_model(model, X_frame, y_frame, split, data_epochs, batch_size, y_scaler):\n",
        "\n",
        "    length = X_frame.shape[0]\n",
        "    X_train = X_frame[:int(length*split),:]\n",
        "    y_train = y_frame[:int(length*split)]\n",
        "\n",
        "    X_test = X_frame[int(length*split):,:]\n",
        "    y_test = y_frame[int(length*split):]\n",
        "\n",
        "    model.fit(X_train, y_train, verbose=0, epochs=data_epochs,\n",
        "                    batch_size=batch_size, validation_split=0.2)\n",
        "    preds = model.predict(X_test, verbose=0)\n",
        "    preds = y_scaler.inverse_transform(preds)\n",
        "    y_test = y_scaler.inverse_transform(y_test)\n",
        "\n",
        "    return mse(y_test, preds, squared=True)\n",
        "\n",
        "\n",
        "def simple_evaluate(future, set_name, X_train, y_train, epochs, batch_size):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "    tf.compat.v1.logging.set_verbosity(30)\n",
        "\n",
        "    model = build_baseline_model()\n",
        "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n",
        "\n",
        "    print(\"Finished evaluating baseline for future {0}\".format(future))\n",
        "\n",
        "    time_end = time.time()\n",
        "    run_time = time_end - time_start\n",
        "\n",
        "    return run_time, model\n",
        "\n",
        "\n",
        "def simple_predict(future, set_name, pred_dates_test, X_test, y_test, y_scaler, model):\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    predictions = y_scaler.inverse_transform(predictions).reshape(-1)\n",
        "    y_test = y_scaler.inverse_transform(y_test).reshape(-1)\n",
        "\n",
        "    make_csvs(csv_directory, predictions, y_test, pred_dates_test, set_name, future, \"Baseline\")\n",
        "\n",
        "    print(\"Finished running baseline prediction on future window {0}\".format(future))\n",
        "\n",
        "    metric_outputs = get_metrics(predictions, y_test, 0, \"Baseline\")\n",
        "    return metric_outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "N6Y8X3gbLq3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collapse_columns(data):\n",
        "    data = data.copy()\n",
        "    if isinstance(data.columns, pd.MultiIndex):\n",
        "        data.columns = data.columns.to_series().apply(lambda x: \"__\".join(x))\n",
        "    return data\n",
        "\n",
        "\n",
        "def create_dataset_2d(input, win_size):\n",
        "\n",
        "    np_data = np.array(input.copy())\n",
        "\n",
        "    X = []\n",
        "\n",
        "    for i in range(len(np_data)-win_size):\n",
        "        row = [r for r in np_data[i:i+win_size]]\n",
        "        X.append(row)\n",
        "\n",
        "    X = np.array(X)\n",
        "    X = X.reshape(X.shape[0], -1)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def create_dataset_3d(input, win_size):\n",
        "\n",
        "    np_data = np.array(input.copy())\n",
        "\n",
        "    X = []\n",
        "\n",
        "    for i in range(len(np_data)-win_size):\n",
        "        row = [r for r in np_data[i:i+win_size]]\n",
        "        X.append(row)\n",
        "\n",
        "    return np.array(X)\n",
        "\n",
        "\n",
        "def load_datasets(csv_directory, set_name, future):\n",
        "\n",
        "    data_name = csv_directory + \"/\" + set_name + \"_data_\" + str(future) + \".csv\"\n",
        "    output_name = csv_directory + \"/\" + set_name + \"_outputs_\" + str(future) + \".csv\"\n",
        "\n",
        "    data = pd.read_csv(data_name).set_index(\"Date\")\n",
        "    outputs = pd.read_csv(output_name).set_index(\"Date\")\n",
        "\n",
        "    return data, outputs\n",
        "\n",
        "\n",
        "def finalise_data(data, outputs, target, best_results):\n",
        "\n",
        "    pred_dates = outputs.index\n",
        "\n",
        "    pca_dim = best_results.get(\"pca_dimensions\")\n",
        "    y_scaler = None\n",
        "\n",
        "    if best_results.get(\"scaler\") == \"minmax\":\n",
        "        X_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "        y_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "        data = X_scaler.fit_transform(data)\n",
        "        outputs = y_scaler.fit_transform(outputs[[target]])\n",
        "\n",
        "    elif best_results.get(\"scaler\") == \"standard\":\n",
        "        X_scaler = StandardScaler()\n",
        "        y_scaler = StandardScaler()\n",
        "        data = X_scaler.fit_transform(data)\n",
        "        outputs = y_scaler.fit_transform(outputs[[target]])\n",
        "\n",
        "    if pca_dim == \"None\":\n",
        "        pca = PCA()\n",
        "        data = pca.fit_transform(data)\n",
        "    elif pca_dim == \"mle\":\n",
        "        pca = PCA(n_components=\"mle\")\n",
        "        data = pca.fit_transform(data)\n",
        "    elif pca_dim != \"NO_PCA\":\n",
        "        pca = PCA(n_components=pca_dim)\n",
        "        data = pca.fit_transform(data)\n",
        "\n",
        "    X_frame = np.array(data)\n",
        "    y_data = np.array(outputs)\n",
        "\n",
        "    return X_frame, y_data, pred_dates, y_scaler\n",
        "\n",
        "\n",
        "def data_cleaning_pipeline(data_in, outputs_in, cleaning_parameters, target, split, data_epochs, batch_size, csv_directory):\n",
        "\n",
        "    best_results = {\"MSE\": [math.inf], \"scaler\": [None], \"pca_dimensions\": [None]}\n",
        "\n",
        "    for scale_type in cleaning_parameters.get('scalers'):\n",
        "            for pca_dim in cleaning_parameters.get('pca_dimensions'):\n",
        "\n",
        "                data = data_in.copy()\n",
        "                outputs = outputs_in.copy()\n",
        "\n",
        "                if scale_type == 'minmax':\n",
        "                    X_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "                    y_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "                    data = X_scaler.fit_transform(data)\n",
        "                    outputs = y_scaler.fit_transform(outputs[[target]])\n",
        "\n",
        "                elif scale_type == 'standard':\n",
        "                    X_scaler = StandardScaler()\n",
        "                    y_scaler = StandardScaler()\n",
        "                    data = X_scaler.fit_transform(data)\n",
        "                    outputs = y_scaler.fit_transform(outputs[[target]])\n",
        "\n",
        "                if pca_dim == None:\n",
        "                    pca = PCA()\n",
        "                    data = pca.fit_transform(data)\n",
        "                elif pca_dim == -math.inf:\n",
        "                    pca = PCA(n_components=\"mle\")\n",
        "                    data = pca.fit_transform(data)\n",
        "                elif pca_dim != math.inf:\n",
        "                    pca = PCA(n_components=pca_dim)\n",
        "                    data = pca.fit_transform(data)\n",
        "\n",
        "                X_frame = np.array(data)\n",
        "                y_frame = np.array(outputs)\n",
        "\n",
        "                model = build_simple_model()\n",
        "                mse = train_simple_model(model, X_frame, y_frame, split, data_epochs, batch_size, y_scaler)\n",
        "                print(\"Trained scale:{0} dim:{1}\".format(scale_type, pca_dim))\n",
        "                if mse < best_results.get(\"MSE\"):\n",
        "                    if pca_dim == None:\n",
        "                        pca_dim = \"None\"\n",
        "                    elif pca_dim == math.inf:\n",
        "                        pca_dim = \"NO_PCA\"\n",
        "                    elif pca_dim == -math.inf:\n",
        "                        pca_dim = \"mle\"\n",
        "                    best_results[\"MSE\"][0] = mse\n",
        "                    best_results[\"pca_dimensions\"][0] = pca_dim\n",
        "                    best_results[\"scaler\"][0] = scale_type\n",
        "\n",
        "    results_data = pd.DataFrame.from_dict(best_results)\n",
        "    results_data.to_csv(csv_directory + \"/best_data_parameters.csv\", index=False)\n",
        "\n",
        "    best_results = {\"MSE\": best_results.get(\"MSE\"), \"scaler\": best_results.get(\"scaler\")[0], \"pca_dimensions\": best_results.get(\"pca_dimensions\")[0]}\n",
        "    return best_results\n",
        "\n",
        "\n",
        "def feature_adder(csv_directory, file_path, target, trend_type, future, epd,  set_name):\n",
        "\n",
        "    data = pd.read_csv(file_path).set_index(\"Dates\")\n",
        "    data = collapse_columns(data)\n",
        "\n",
        "    data['PrevDaySameHour'] = data[target].copy().shift(epd)\n",
        "    data['PrevWeekSameHour'] = data[target].copy().shift(epd*7)\n",
        "    data['Prev24HourAveLoad'] = data[target].copy().rolling(window=epd*7, min_periods=1).mean()\n",
        "\n",
        "    try:\n",
        "        data['Weekday'] = data.index.dayofweek\n",
        "        if 'Holiday' in data.columns.values:\n",
        "            data.loc[(data['Weekday'] < 5) & (data['Holiday'] == 0), 'IsWorkingDay'] = 1\n",
        "            data.loc[(data['Weekday'] > 4) | (data['Holiday'] == 1), 'IsWorkingDay'] = 0\n",
        "        else:\n",
        "            data.loc[data['Weekday'] < 5, 'IsWorkingDay'] = 1\n",
        "            data.loc[data['Weekday'] > 4, 'IsWorkingDay'] = 0\n",
        "    except AttributeError:\n",
        "        pass\n",
        "\n",
        "    dec_daily = seasonal_decompose(data[target], model=trend_type, period=epd)\n",
        "    data['IntraDayTrend'] = dec_daily.trend\n",
        "    data['IntraDaySeasonal'] = dec_daily.seasonal\n",
        "    data['IntraDayTrend'] = data['IntraDayTrend'].shift(epd)\n",
        "    data['IntraDaySeasonal'] = data['IntraDaySeasonal'].shift(epd)\n",
        "\n",
        "    dec_weekly = seasonal_decompose(data[target], model=trend_type, period=epd*7)\n",
        "    data['IntraWeekTrend'] = dec_weekly.trend\n",
        "    data['IntraWeekSeasonal'] = dec_weekly.seasonal\n",
        "    data['IntraWeekTrend'] = data['IntraWeekTrend'].shift(epd*7)\n",
        "    data['IntraWeekSeasonal'] = data['IntraWeekSeasonal'].shift(epd*7)\n",
        "\n",
        "    data[target] = y = data[target].shift(-future)\n",
        "    data = data.dropna(how='any', axis='rows')\n",
        "    y = data[target].reset_index(drop=True)\n",
        "\n",
        "    future_dates = pd.Series(data.index[future:])\n",
        "    outputs = pd.DataFrame({\"Date\": future_dates, \"{0}\".format(target): y})\n",
        "\n",
        "    data = data.drop(\"{0}\".format(target), axis=1)\n",
        "\n",
        "    data_name = csv_directory + \"/\" + set_name + \"_data_\" + str(future) + \".csv\"\n",
        "    output_name = csv_directory + \"/\" + set_name + \"_outputs_\" + str(future) + \".csv\"\n",
        "\n",
        "    data.to_csv(data_name)\n",
        "    outputs.to_csv(output_name, index=False)\n",
        "\n",
        "    print(\"Saved future window {0} to csvs\".format(future))\n",
        "\n",
        "    return data, outputs"
      ],
      "metadata": {
        "id": "3f3frZJSMq_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bnn_kt_model(hp):\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    hp_reg = hp.Float(\"reg\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    hp_dropout = hp.Float(\"dropout\", min_value=1e-3, max_value=0.5, sampling=\"linear\")\n",
        "    hp_neuron_pct = hp.Float('NeuronPct', min_value=1e-3, max_value=1.0, sampling='linear')\n",
        "    hp_neuron_shrink = hp.Float('NeuronShrink', min_value=1e-3, max_value=1.0, sampling='linear')\n",
        "\n",
        "    hp_max_neurons = hp.Int('neurons', min_value=10, max_value=200, step=10)\n",
        "\n",
        "    neuron_count = int(hp_neuron_pct * hp_max_neurons)\n",
        "    layers = 0\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    while neuron_count > 5 and layers < 5:\n",
        "\n",
        "        model.add(Dense(units=neuron_count, activation=hp_activation))\n",
        "        model.add(Dropout(hp_dropout))\n",
        "        layers += 1\n",
        "        neuron_count = int(neuron_count * hp_neuron_shrink)\n",
        "\n",
        "    model.add(Dense(1, 'linear'))\n",
        "\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=hp_learning_rate),\n",
        "                metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def bnn_save_plots(history, graphs_directory, set_name, future):\n",
        "\n",
        "    graph_names = {\"Loss\": \"loss\", \"MAE\": \"mean_absolute_error\",\n",
        "                   \"MSE\": \"mean_squared_error\", \"MAPE\": \"mean_absolute_percentage_error\"}\n",
        "\n",
        "    for name, value in graph_names.items():\n",
        "        graph_loc = graphs_directory + \"/\" + set_name + \"_basic_nn_\" + str(future) + \"_\" + name + \".png\"\n",
        "        if os.path.exists(graph_loc):\n",
        "            os.remove(graph_loc)\n",
        "\n",
        "        val_name = \"val_\" + value\n",
        "        plt.plot(history.history[value])\n",
        "        plt.plot(history.history[val_name])\n",
        "        plt.title('Basic NN {0} for {1} {2}'.format(name, set_name, future))\n",
        "        plt.ylabel(name)\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.savefig(graphs_directory + \"/\" + set_name + \"_basic_nn_\" + str(future) + \"_\" + name + \".png\")\n",
        "\n",
        "\n",
        "def bnn_train_model(future, batch_size, epochs,\n",
        "                model_directory, set_name, X_train, y_train, y_scaler, epd):\n",
        "\n",
        "    tuner = kt.Hyperband(bnn_kt_model, objective='mean_absolute_percentage_error', max_epochs=epochs, factor=3,\n",
        "                        directory=model_directory + \"/\" + set_name + \"_kt_dir\", project_name='kt_model_' + str(future),\n",
        "                        overwrite=True)\n",
        "\n",
        "    monitor = EarlyStopping(monitor='mean_absolute_percentage_error', min_delta=1, patience=5, verbose=0, mode='auto',\n",
        "                    restore_best_weights=True)\n",
        "\n",
        "    tuner.search(X_train, y_train, verbose=0, epochs=epochs, validation_split=0.2, batch_size=batch_size,\n",
        "                callbacks=[monitor])\n",
        "\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "    # Split on a 3 monthly basis\n",
        "    # tss = TimeSeriesSplit(n_splits=n_splits, test_size=epd*cv_period, gap=0)\n",
        "    # fold = 0\n",
        "    # total_metrics = {}\n",
        "\n",
        "    # for train_idx, val_idx in tss.split(X_train, y_train):\n",
        "\n",
        "    #     fold_name = \"Fold_\" + str(fold)\n",
        "    #     X_t = X_train[train_idx]\n",
        "    #     X_v = X_train[val_idx]\n",
        "    #     y_t = y_train[train_idx]\n",
        "    #     y_v = y_train[val_idx]\n",
        "\n",
        "    #     if fold == 2:\n",
        "    #         history = model.fit(X_t, y_t, verbose=0, epochs=epochs, callbacks=[monitor],\n",
        "    #                 batch_size=batch_size, validation_data=(X_v, y_v))\n",
        "    #         graphs_directory = os.getcwd() + \"/graphs\"\n",
        "    #         bnn_save_plots(history, graphs_directory, set_name, future)\n",
        "    #         model.save(model_directory + \"/\" + set_name + \"_basic_nn_\" + str(future))\n",
        "\n",
        "    #     model.fit(X_t, y_t, verbose=0, epochs=epochs, callbacks=[monitor],\n",
        "    #                 batch_size=batch_size)\n",
        "    #     preds = model.predict(X_v, verbose=0)\n",
        "    #     preds = y_scaler.inverse_transform(preds)\n",
        "    #     metrics = get_metrics(preds, y_v, 1, \"Basic_nn\")\n",
        "    #     total_metrics[fold_name] = metrics\n",
        "\n",
        "    #     fold += 1\n",
        "\n",
        "    # cross_val_metrics(total_metrics, set_name, future, \"Basic_nn\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def bnn_predict(future, set_name, pred_dates_test, X_test, y_test, y_scaler, model):\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    predictions = y_scaler.inverse_transform(predictions).reshape(-1)\n",
        "    y_test = y_scaler.inverse_transform(y_test).reshape(-1)\n",
        "\n",
        "    make_csvs(csv_directory, predictions, y_test, pred_dates_test, set_name, future, \"Basic_nn\")\n",
        "\n",
        "    print(\"Finished running basic prediction on future window {0}\".format(future))\n",
        "\n",
        "    metric_outputs = get_metrics(predictions, y_test, 0, \"Basic_nn\")\n",
        "    return metric_outputs\n",
        "\n",
        "\n",
        "def bnn_evaluate(future, set_name, X_train, y_train, epochs, batch_size, y_scaler, epd, model_directory):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "    tf.compat.v1.logging.set_verbosity(30)\n",
        "\n",
        "    model = bnn_train_model(future, batch_size, epochs,\n",
        "                model_directory, set_name, X_train, y_train, y_scaler, epd)\n",
        "\n",
        "    print(\"Finished evaluating basic nn for future {0}\".format(future))\n",
        "\n",
        "    time_end = time.time()\n",
        "    run_time = time_end - time_start\n",
        "\n",
        "    return run_time, model\n",
        "\n"
      ],
      "metadata": {
        "id": "kls_2MBLNa1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_kt_model(hp):\n",
        "\n",
        "    X = np.load(\"X_train_3d.npy\")\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    hp_dropout = hp.Float(\"dropout\", min_value=1e-3, max_value=0.5, sampling=\"linear\")\n",
        "    hp_neuron_pct = hp.Float('NeuronPct', min_value=1e-3, max_value=1.0, sampling='linear')\n",
        "    hp_neuron_shrink = hp.Float('NeuronShrink', min_value=1e-3, max_value=1.0, sampling='linear')\n",
        "    hp_filter = hp.Int(\"Filter\", min_value=2, max_value=128, sampling=\"linear\")\n",
        "    hp_kernel = hp.Int(\"Kernel\", min_value=1, max_value=min(X.shape[1], X.shape[2]), sampling=\"linear\")\n",
        "\n",
        "    hp_max_neurons = hp.Int('neurons', min_value=10, max_value=5000, step=10)\n",
        "\n",
        "    neuron_count = int(hp_neuron_pct * hp_max_neurons)\n",
        "    layers = 0\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(InputLayer((X.shape[1], X.shape[2])))\n",
        "\n",
        "    model.add(Conv1D(filters=hp_filter, kernel_size=hp_kernel, activation=hp_activation))\n",
        "\n",
        "    model.add(Dropout(hp_dropout))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    while neuron_count > 20 and layers < 20:\n",
        "\n",
        "        model.add(Dense(units=neuron_count, activation=hp_activation))\n",
        "        model.add(Dropout(hp_dropout))\n",
        "        layers += 1\n",
        "        neuron_count = int(neuron_count * hp_neuron_shrink)\n",
        "\n",
        "    model.add(Dense(1, 'linear'))\n",
        "\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=hp_learning_rate),\n",
        "                metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def cnn_save_plots(history, graphs_directory, set_name, future):\n",
        "\n",
        "    graph_names = {\"Loss\": \"loss\", \"MAE\": \"mean_absolute_error\",\n",
        "                   \"MSE\": \"mean_squared_error\", \"MAPE\": \"mean_absolute_percentage_error\"}\n",
        "\n",
        "    for name, value in graph_names.items():\n",
        "        graph_loc = graphs_directory + \"/\" + set_name + \"_cnn_\" + str(future) + \"_\" + name + \".png\"\n",
        "        if os.path.exists(graph_loc):\n",
        "            os.remove(graph_loc)\n",
        "\n",
        "        val_name = \"val_\" + value\n",
        "        plt.plot(history.history[value])\n",
        "        plt.plot(history.history[val_name])\n",
        "        plt.title('cnn {0} for {1} {2}'.format(name, set_name, future))\n",
        "        plt.ylabel(name)\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.savefig(graphs_directory + \"/\" + set_name + \"_cnn_\" + str(future) + \"_\" + name + \".png\")\n",
        "\n",
        "\n",
        "def cnn_train_model(future, batch_size, epochs,\n",
        "                model_directory, set_name, X_train, y_train, y_scaler, epd):\n",
        "\n",
        "    tuner = kt.Hyperband(cnn_kt_model, objective='mean_absolute_percentage_error', max_epochs=epochs, factor=3,\n",
        "                        directory=model_directory + \"/\" + set_name + \"_kt_dir\", project_name='kt_model_' + str(future),\n",
        "                        overwrite=True)\n",
        "\n",
        "    monitor = EarlyStopping(monitor='mean_absolute_percentage_error', min_delta=1, patience=5, verbose=0, mode='auto',\n",
        "                    restore_best_weights=True)\n",
        "\n",
        "    tuner.search(X_train, y_train, verbose=0, epochs=epochs, validation_split=0.2, batch_size=batch_size,\n",
        "                callbacks=[monitor])\n",
        "\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "    # Split on a 3 monthly basis\n",
        "    # tss = TimeSeriesSplit(n_splits=n_splits, test_size=epd*cv_period, gap=0)\n",
        "    # fold = 0\n",
        "    # total_metrics = {}\n",
        "\n",
        "    # for train_idx, val_idx in tss.split(X_train, y_train):\n",
        "\n",
        "    #     fold_name = \"Fold_\" + str(fold)\n",
        "    #     X_t = X_train[train_idx]\n",
        "    #     X_v = X_train[val_idx]\n",
        "    #     y_t = y_train[train_idx]\n",
        "    #     y_v = y_train[val_idx]\n",
        "\n",
        "    #     if fold == 9:\n",
        "    #         history = model.fit(X_t, y_t, verbose=0, epochs=epochs, callbacks=[monitor],\n",
        "    #                 batch_size=batch_size, validation_data=(X_v, y_v))\n",
        "    #         graphs_directory = os.getcwd() + \"/graphs\"\n",
        "    #         cnn_save_plots(history, graphs_directory, set_name, future)\n",
        "    #         model.save(model_directory + \"/\" + set_name + \"_cnn_\" + str(future))\n",
        "\n",
        "    #     model.fit(X_t, y_t, verbose=0, epochs=epochs, callbacks=[monitor],\n",
        "    #                 batch_size=batch_size)\n",
        "    #     preds = model.predict(X_v, verbose=0)\n",
        "    #     preds = y_scaler.inverse_transform(preds)\n",
        "    #     metrics = get_metrics(preds, y_v, 1, \"cnn\")\n",
        "    #     total_metrics[fold_name] = metrics\n",
        "\n",
        "    #     fold += 1\n",
        "\n",
        "    # cross_val_metrics(total_metrics, set_name, future, \"cnn\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def cnn_predict(future, set_name, pred_dates_test, X_test, y_test, y_scaler, model):\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    predictions = y_scaler.inverse_transform(predictions).reshape(-1)\n",
        "    y_test = y_scaler.inverse_transform(y_test).reshape(-1)\n",
        "\n",
        "    make_csvs(csv_directory, predictions, y_test, pred_dates_test, set_name, future, \"cnn\")\n",
        "\n",
        "    print(\"Finished running cnn prediction on future window {0}\".format(future))\n",
        "\n",
        "    metric_outputs = get_metrics(predictions, y_test, 0, \"cnn\")\n",
        "    return metric_outputs\n",
        "\n",
        "\n",
        "def cnn_evaluate(future, set_name, X_train, y_train, epochs, batch_size, y_scaler, epd, model_directory):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "    tf.compat.v1.logging.set_verbosity(30)\n",
        "\n",
        "    model = cnn_train_model(future, batch_size, epochs,\n",
        "              model_directory, set_name, X_train, y_train, y_scaler, epd)\n",
        "\n",
        "    print(\"Finished evaluating cnn for future {0}\".format(future))\n",
        "\n",
        "    time_end = time.time()\n",
        "    run_time = time_end - time_start\n",
        "\n",
        "    return run_time, model"
      ],
      "metadata": {
        "id": "RD29RxrNPxzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_kt_model(hp):\n",
        "\n",
        "    X = np.load(\"X_train_3d.npy\")\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    hp_reg = hp.Float(\"reg\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    hp_dropout = hp.Float(\"dropout\", min_value=1e-3, max_value=0.5, sampling=\"linear\")\n",
        "    hp_neuron_pct = hp.Float('NeuronPct', min_value=1e-3, max_value=1.0, sampling='linear')\n",
        "    hp_neuron_shrink = hp.Float('NeuronShrink', min_value=1e-3, max_value=1.0, sampling='linear')\n",
        "\n",
        "    hp_l_layer_1 = hp.Int('l_layer_1', min_value=1, max_value=100, step=10)\n",
        "    hp_max_neurons = hp.Int('neurons', min_value=10, max_value=5000, step=10)\n",
        "\n",
        "    neuron_count = int(hp_neuron_pct * hp_max_neurons)\n",
        "    layers = 0\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(InputLayer((X.shape[1], X.shape[2])))\n",
        "    model.add(LSTM(hp_l_layer_1, return_sequences=True, activity_regularizer=regularizers.l1(hp_reg)))\n",
        "    model.add(Dropout(hp_dropout))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    while neuron_count > 20 and layers < 20:\n",
        "\n",
        "        model.add(Dense(units=neuron_count, activation=hp_activation))\n",
        "        model.add(Dropout(hp_dropout))\n",
        "        layers += 1\n",
        "        neuron_count = int(neuron_count * hp_neuron_shrink)\n",
        "\n",
        "    model.add(Dense(1, 'linear'))\n",
        "\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=hp_learning_rate),\n",
        "                metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def lstm_save_plots(history, graphs_directory, set_name, future):\n",
        "\n",
        "    graph_names = {\"Loss\": \"loss\", \"MAE\": \"mean_absolute_error\",\n",
        "                   \"MSE\": \"mean_squared_error\", \"MAPE\": \"mean_absolute_percentage_error\"}\n",
        "\n",
        "    for name, value in graph_names.items():\n",
        "        graph_loc = graphs_directory + \"/\" + set_name + \"_lstm_\" + str(future) + \"_\" + name + \".png\"\n",
        "        if os.path.exists(graph_loc):\n",
        "            os.remove(graph_loc)\n",
        "\n",
        "        val_name = \"val_\" + value\n",
        "        plt.plot(history.history[value])\n",
        "        plt.plot(history.history[val_name])\n",
        "        plt.title('LSTM {0} for {1} {2}'.format(name, set_name, future))\n",
        "        plt.ylabel(name)\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.savefig(graphs_directory + \"/\" + set_name + \"_lstm_\" + str(future) + \"_\" + name + \".png\")\n",
        "\n",
        "\n",
        "def lstm_train_model(future, batch_size, epochs,\n",
        "                model_directory, set_name, X_train, y_train, y_scaler, epd):\n",
        "\n",
        "    tuner = kt.Hyperband(lstm_kt_model, objective='mean_absolute_percentage_error', max_epochs=epochs, factor=3,\n",
        "                        directory=model_directory + \"/\" + set_name + \"_kt_dir\", project_name='kt_model_' + str(future),\n",
        "                        overwrite=True)\n",
        "\n",
        "    monitor = EarlyStopping(monitor='mean_absolute_percentage_error', min_delta=1, patience=5, verbose=0, mode='auto',\n",
        "                    restore_best_weights=True)\n",
        "\n",
        "    tuner.search(X_train, y_train, verbose=0, epochs=epochs, validation_split=0.2, batch_size=batch_size,\n",
        "                callbacks=[monitor])\n",
        "\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "    # Split on a 3 monthly basis\n",
        "    # tss = TimeSeriesSplit(n_splits=n_splits, test_size=epd*cv_period, gap=0)\n",
        "    # fold = 0\n",
        "    # total_metrics = {}\n",
        "\n",
        "    # for train_idx, val_idx in tss.split(X_train, y_train):\n",
        "\n",
        "    #     fold_name = \"Fold_\" + str(fold)\n",
        "    #     X_t = X_train[train_idx]\n",
        "    #     X_v = X_train[val_idx]\n",
        "    #     y_t = y_train[train_idx]\n",
        "    #     y_v = y_train[val_idx]\n",
        "\n",
        "    #     if fold == 9:\n",
        "    #         history = model.fit(X_t, y_t, verbose=0, epochs=epochs, callbacks=[monitor],\n",
        "    #                 batch_size=batch_size, validation_data=(X_v, y_v))\n",
        "    #         graphs_directory = os.getcwd() + \"/graphs\"\n",
        "    #         lstm_save_plots(history, graphs_directory, set_name, future)\n",
        "    #         model.save(model_directory + \"/\" + set_name + \"_lstm_\" + str(future))\n",
        "\n",
        "    #     model.fit(X_t, y_t, verbose=0, epochs=epochs, callbacks=[monitor],\n",
        "    #                 batch_size=batch_size)\n",
        "    #     preds = model.predict(X_v, verbose=0)\n",
        "    #     preds = y_scaler.inverse_transform(preds)\n",
        "    #     metrics = get_metrics(preds, y_v, 1, \"lstm\")\n",
        "    #     total_metrics[fold_name] = metrics\n",
        "\n",
        "    #     fold += 1\n",
        "\n",
        "    # cross_val_metrics(total_metrics, set_name, future, \"lstm\")\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def lstm_predict(future, set_name, pred_dates_test, X_test, y_test, y_scaler, model):\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    predictions = y_scaler.inverse_transform(predictions).reshape(-1)\n",
        "    y_test = y_scaler.inverse_transform(y_test).reshape(-1)\n",
        "\n",
        "    make_csvs(csv_directory, predictions, y_test, pred_dates_test, set_name, future, \"lstm\")\n",
        "\n",
        "    print(\"Finished running lstm prediction on future window {0}\".format(future))\n",
        "\n",
        "    metric_outputs = get_metrics(predictions, y_test, 0, \"lstm\")\n",
        "    return metric_outputs\n",
        "\n",
        "\n",
        "def lstm_evaluate(future, set_name, X_train, y_train, epochs, batch_size, y_scaler, epd, model_directory):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "    tf.compat.v1.logging.set_verbosity(30)\n",
        "\n",
        "    model = lstm_train_model(future, batch_size, epochs,\n",
        "                model_directory, set_name, X_train, y_train, y_scaler, epd)\n",
        "\n",
        "    print(\"Finished evaluating lstm for future {0}\".format(future))\n",
        "\n",
        "    time_end = time.time()\n",
        "    run_time = time_end - time_start\n",
        "\n",
        "    return run_time, model"
      ],
      "metadata": {
        "id": "B0UCgai1Qzoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TltMEKgvLQJC"
      },
      "outputs": [],
      "source": [
        "  csv_directory = \"\"\n",
        "  model_directory = \"\"\n",
        "\n",
        "  # These are the values that need changing per different dataset\n",
        "  file_path = csv_directory + \"aemo_nsw_target_only.csv\"\n",
        "  set_name = \"AEMO\"\n",
        "  target = \"TOTALDEMAND\"\n",
        "  trend_type = \"Additive\"\n",
        "  epd = 288\n",
        "  future = 288\n",
        "\n",
        "  partition = 5000\n",
        "  data_epochs = 10\n",
        "\n",
        "  cleaning_parameters = {\n",
        "      'pca_dimensions': [None, math.inf, -math.inf],\n",
        "      'scalers': ['standard', 'minmax']\n",
        "  }\n",
        "\n",
        "  window = 10\n",
        "  split = 0.8\n",
        "  epochs = 1\n",
        "  batch_size = 32\n",
        "\n",
        "  data, outputs = feature_adder(csv_directory, file_path, target, trend_type, future, epd,  set_name)\n",
        "  best_results = data_cleaning_pipeline(data[:partition], outputs[:partition], cleaning_parameters, target, split, data_epochs, batch_size, csv_directory)\n",
        "\n",
        "  print(\"finished cleaning\")\n",
        "  X_frame, y_data, pred_dates, y_scaler = finalise_data(data, outputs, target, best_results)\n",
        "  length = X_frame.shape[0]\n",
        "\n",
        "  pred_dates_test = pred_dates[int(length*split) + window:]\n",
        "\n",
        "  X_2d = create_dataset_2d(X_frame, window)\n",
        "  X_3d = create_dataset_3d(X_frame, window)\n",
        "\n",
        "  y_test = y_data[int(length*split) + window:]\n",
        "  X_test_2d = X_2d[int(length*split):]\n",
        "  X_test_3d = X_3d[int(length*split):]\n",
        "\n",
        "  y_train = np.ravel(y_data[window:int(length*split) + window])\n",
        "  X_train_2d = X_2d[:int(length * split)]\n",
        "  X_train_3d = X_3d[:int(length * split)]\n",
        "\n",
        "  np.save(\"X_train_3d.npy\", X_train_3d)\n",
        "\n",
        "  # Basic NN\n",
        "  run_time, model = bnn_evaluate(future, set_name, X_train_2d, y_train, epochs, epd, model_directory)\n",
        "  metrics = bnn_predict(future, set_name, pred_dates_test, X_test_2d, y_test, y_scaler, model)\n",
        "\n",
        "  # LSTM\n",
        "  # run_time, model = lstm_evaluate(future, set_name, X_train_2d, y_train, epochs, epd, model_directory)\n",
        "  # metrics = lstm_predict(future, set_name, pred_dates_test, X_test_2d, y_test, y_scaler, model)\n",
        "\n",
        "  # CNN\n",
        "  # run_time, model = cnn_evaluate(future, set_name, X_train_2d, y_train, epochs, epd, model_directory)\n",
        "  # metrics = cnn_predict(future, set_name, pred_dates_test, X_test_2d, y_test, y_scaler, model)\n",
        "\n",
        "  # Base model\n",
        "  # run_time, model = simple_evaluate(future, set_name, X_train_2d, y_train, epochs, batch_size)\n",
        "  # metrics = simple_predict(future, set_name, pred_dates_test, X_test_2d, y_test, y_scaler, model)\n",
        "\n",
        "  metrics['TIME'] = run_time\n",
        "  metrics = {\"model\": metrics}\n",
        "  # metrics = normalise_metrics(metrics, training)\n",
        "\n",
        "  make_metrics_csvs(csv_directory, metrics, set_name, future, 1)\n",
        "\n",
        "  if os.path.exists(\"X_train_3d.npy\"):\n",
        "      os.remove(\"X_train_3d.npy\")\n"
      ]
    }
  ]
}